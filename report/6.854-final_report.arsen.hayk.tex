\documentclass{article}

\usepackage{graphicx}
\usepackage[shortlabels]{enumitem}


\newcommand{\twopartdef}[4]
{
    \left\{
    \begin{array}{ll}
        #1 & #2 \\
        #3 & #4
    \end{array}
    \right.
}

\newcommand{\threepartdef}[6]
{
    \left\{
    \begin{array}{ll}
        #1 & #2 \\
        #3 & #4 \\
        #5 & #6
    \end{array}
    \right.
}

\begin{document}

\title{Time Range Queries for Hereditary Properties \\ \large 6.854 Final Project}
\author{Arsen Mamikonyan, Hayk Saribekyan}

\maketitle

\begin{abstract}
    Time range queries are important for analysing large datasets with timestamps. Such datasets are common in computational geometry, where a sequence of points with timestamps often corresponds to a trajectory of an object. Time range queries in this case check a certain property of section of the trajectory.
    
    In this paper we review recent general frameworks that can be used to handle property testing queries for time ranges, and discuss their implementations for solving certain problems in computational geometry. The results are limited to \textit{hereditary} properties, which cover a large variety of interesting problems.
    
    For two different hereditary properties, we compared the performance of efficient an algorithm and a naive one in practice. We observed that the efficient algorithms require more than ten times the amount of code in certain cases, but the large constant factor associated with it is compensated when working with large datasets.
\end{abstract}

\section{Introduction}
\label{sec:intro}
TODO: replace all arch to arc, arch is a part of a bridge, arc is a part of a circle\\

TODO: add normal references (maybe to envelopes), see the first paper, where in the first page there are a lot of motivational references.


With the abundance of GPS and other movement tracking sensors there is a large amount of timestamped location data. The movement of an object in space can be modelled by a sequence of points $S = s_1, \dots, s_n$. To analyse the trajectory of the movement, one may want to check a certain property $P$ for portions $S[i, j] = s_i, \dots, s_j$ of $S$ for given values of $i$ and $j$. This paper discusses how to efficiently address such queries when $P$ satisfies the following two properties:
\begin{itemize}
    \item $P$ is boolean.
    \item $P$ is hereditary. This means that for a given sequence $S$, if $P(S)$ is true, then $P(S')$ is true for any continuous subsequence $S'$ of $S$.
\end{itemize}

In this paper we discuss and implement two properties for a sequence of points $S = s_1, \dots, s_n$:
\begin{itemize}
    \item \textit{Monotonicity}:  $S$ is \textit{monotone} if there is a direction vector $v$ such that TODO. In terms of trajectories, monotonicity shows whether the object travelled more or less in the same direction.
    
    \item \textit{Closeness}: $S$ is \textit{close} if any pair of points in $S$ is at most of distance $1$. One could use closeness queries to detect if a moving object mostly stayed in the same surrounding.
\end{itemize}

For a property $P$ satisfying the restrictions above, let $j^*(i)$ be the largest index $j$ such that $P$ holds for $S[i, j^*(i)] = s_i, \dots, s_{j^*(i)}$. Now notice, that since $P$ is a hereditary property $P(S[i, j]) = true$ if and only if $j \leq j^*(i)$. Therefore, if we construct $j^*(i)$ we can answer property testing queries in $O(1)$. Therefore, for the rest of the paper our goal will be to efficiently compute $j^*(i)$.

Bokal et al. \cite{bokal2015} propose an algorithms that compute $j^*(i)$ in $O(n)$ time for monotonicity, and in $O(n\log^2 n)$ time for closeness. Chan and Pratt \cite{chan2016} describe a different algorithm to achieve the same result for closeness, but also improve it to $O(n\log n)$ using fractional cascading.

The rest of the paper is organized as follows: Section \ref{sec:naive} describes the naive algorithms we developed to solve the problems we selected; in Section \ref{sec:monotonicity} we review the general framework that is set in \cite{bokal2015} for solving time range query problems, and show how it is applied to computer $j^*(i)$ for monotonicity. Section \ref{sec:closeness} reviews the framework by \cite{chan2016} and its application for the closeness property. We then give some implementation details in Section \cite{sec:implementation}, and describe the experimental setup and our tests in Section \cite{sec:experiments}.

\section{Naive Algorithms}
\label{sec:naive}
Suppose that for a sequence of length $n$ a property $P$ can be checked in $T_P(n)$ time. Then, one can compute $j^*(i)$ using binary search for each $i$. On a step of a binary search concerning a range $[l, r]$ the algorithm would spend $T_P(r - l)$ time. The total runtime of this simple algorithm is $O(nT_P(n)\log n)$. Call this algorithm $SN$ (stands for super-naive). For the problems of monotonicity and closeness we can do better.

\subsection{Monotonicity}
\label{sec:naive:monotonicity}
To detect whether a sequence is monotone or no, we can process the points while keeping a set of polar angles (field-of-view or FoV) that a direction vector $v$ can have. Notice that the FoV is always an interval. When a new point arrives we can update the FoV in constant time. If the FoV ever becomes empty, then the given sequence is not monotone. This algorithm runs in $O(n)$ time for a sequence of length $n$. Using this submodule for $SN$ we would get a $O(n^2\log n)$ algorithm.

However, doing a binary search for monotonicity is redundant, since we will do the same computation many times. Instead, for each $i$ proceed with the FoV computation until it is empty, which will happen precisely when we reach $j^*(i) + 1$. The resulting algorithm runs in $O(n^2)$ time.

\subsection{Closeness}
\label{sec:naive:closeness}
The simple method of closeness-check takes $O(n^2)$ for a sequence of $n$ points, because one has to check all pairs of points. This results in $SN$ runtime of $O(n^3\log n)$, which is super-slow. We can improve the closeness-check to $O(n\log n)$ using furthest-point Voronoi diagrams, but that would not qualify for $SN$ and would also take $O(n^2 \log^2 n)$ time to compute, which is also not fast.

Define $k^*(i)$ as the largest index such that $d(s_i, s_j) \leq 1$ for all $j \leq k*(i)$. Clearly, $k^*(i) \geq j^*(i)$. Also, notice that $k^*$ can be computed in $O(n^2)$ for all points.

\textbf{Claim TODO - formatting} For $i < n$ $j^*(i) = \min( j^*(i + 1), k^*(i) )$

\textbf{Proof} obvious (should we even prove this?

This claim gives us an $O(n^2)$ $SN$ construction of $j^*(i)$ for the closeness property.

\section{something and Monotonicity TODO}
\label{sec:monotonicity}
Bokal et al. introduced an elegant framework to deal with range queries for hereditary problems. The key idea in their work is to greedily split the given set of points into ranges, defined by anchor points, and solve for each using a divide and conquer. To simplify the reasoning we first define a property matrix $A_P$ such that
\[ A_P(i, j) = \twopartdef{1}{P(S[i, j]) = true}{0}{P(S[i, j]) = false} \]

See Figure \ref{fig:property_matrix} for an example. Suppose that there is an algorithm $J_P$ that, for a given $i$, finds $j^*(i)$ (notice that even if such $J_P$ exists, we are not happy to run it $n$ times for each $i$). We use $J_P$ to find \textit{anchor} points in $S$ as follows. Let $a_k$ be the index of $k^{th}$ anchor point in $S$. We define $a_1 = s_1$, $a_k = j^*(a_{k-1})$ for $k > 1$. Thus, the anchor points can be found using $J_P$. Figure \ref{fig:property_matrix} shows the anchor points on $A$ (leftmost image).

\begin{figure}
    \centering
    \makebox[\textwidth][c]{\includegraphics[width=1.2\textwidth]{figures/property_matrix.pdf}}
    \label{fig:property_matrix}
    \caption{The property matrix $A_P$. The blue regions show the locations that are 1. The left image illustrates the anchor points. The middle one shows the greedily constructed rectangles from the anchor points. On the right side is the detailed view of the property matrix. Figure from \cite{bokal2015}.}
\end{figure}

Also shown in Figure \ref{fig:property_matrix} are rectangles formed using the anchor points. If we solve a rectangle (find the curve in $A$ separating 1's from 0's) for each rectangle, then we will be done, as stated in Lemma 2.1 from Bokal et al.

\textbf{Lemma 2.1} Assume that we have the following two subroutines for sequence $S$ and a property $P$:
\begin{enumerate}[(a)]
    \item For any $a = 1, \dots, n$ find the $j^*(a)$, taking $T_{greedy}(j^*(a) - a)$
    \item Solve a rectangle with anchor points at opposite vertices (anchored rectangles). Suppose this module takes $T_{rect}(height(R) + width(R))$.
\end{enumerate}

Both of these modules are property specific, but Bokal et al. make another interesting generic step that explains how solving rectangles can be done using a divide-and-conquer approach. We will, however, skip that step and focus on the problem of monotonicity. It turns out that monotonicity does not require the divide-and-conquer approach to solve anchored rectangles.

\subsection{Solving Monotonicity}
Section \ref{sec:naive:monotonicity} discusses how to find $j^*(i)$ in linear time for a given $i$ by using intervals of polar angles. This algorithm will serve as the subroutine for Lemma \ref{lemma2.1}(a).

Consider an anchored rectangle $R$ with a lower-left corner $(a, a)$, width $w$ and height $h$. By traversing the points $S[a, a + w]$, we can compute for each $j = a, \dots, a + w$ the interval of polar angles $I_j$ such that $S[a, j]$ is monotone with respect to any direction in $I_j$. Also, notice that by traversing the points in $S[a - h, a]$ in reverse order, a similar set $I_i$ can be computed for $a - h \leq i < a$, representing the set of monotone directions for $S[i, a]$.

Notice that if $i < a < j$, then $S[i, j]$ is monotone if and only if $I_i$ and $I_j$ have a non-empty intersection. Thus, for each $i = a - h, \dots, a$, $j^*(i)$ is the largest index such that $I_i \cap I_{j^*(i)} \neq \emptyset$. Since the search for $j^*(i+1)$ can start from $j^*(i)$, we can compute $j^*(i)$ for all $i \in [a - h, a]$ in linear time.

These two subroutines, together with \ref{lemma2.1} give a linear solution to the monotonicity problems.

%This is where divide and conquer strategy is used. Consider a rectangle $R$ with rows $[a, a + h]$ and columns $[b, b + w]$. We can set $m = a + [h / 2]$ and using $J_P$ find $j^*(m)$. If $m \leq i \leq a + h$ then $j^*(i) \geq j^*(m)$, and if $a \leq i \leq m$ then $j^*(i) \leq j^*(m)$. Therefore, once we know the value of $j^*(m)$ the solution for $R$ can be computed using two recursive calls to its upper-left and lower-right sub-rectangles.

%Notice, however, this type of recursion will just end up calling $J_P$ for all indices in $[a, a + h]$, having no divide-and-conquer effect.



\section{something and Closeness TODO}
\label{sec:closeness}
TODO: Arsen, this section should describe some preliminaries of the algorithm (e.g. the notion of polyarcs), so that there is coherency between the sections.

\section{Implementation Details}
\label{sec:implementation}
In the papers reviewed here (\cite{bokal2015,chan2016}) there are at least 6 algorithms presented. We wanted to implement algorithm that use frameworks of both papers. In order to limit the size of the implementation we decided to pick monotonicity property for its simplicity (and elegance), which was solved in Bokal et al. \cite{bokal2015}. We chose to implement the closeness testing algorithm from \cite{chan2016} because it used many ideas from the class, for example the intersection of convex envelopes in 2 dimensions, range trees.

The implementations of naive algorithms as well as of the efficient monotonicity testing algorithm are relatively easy, so in this paper we skip discussing them. We implemented the $O(n \log ^2 n)$ algorithm for solving closeness testing from \cite{chan2016}.

The algorithm has two distinct parts: a high-level tree-range data structure, and a lower-level data structure for storing \textit{polyarcs} (see Section \ref{sec:closeness}). A polyarc is a convex geometric shape, which is defined by a clockwise set of its vertices like a polygon. Unlike a polygon, each edge of a polyarc is an arc of a circle. The algorithm needs to be able to find the intersection of two polyarcs, and also decide whether a query point is inside the polyarc.

Both of these algorithms are similar to those of polygons, but the majority of the implementation effort was spent to handle the cases specific to polyarcs. Our representation of polyarcs has two ``sides'': upper and lower, respectively the upper part of the complex shape and the lower one. To intersect two polyarcs we first found a minimal upper envelope i.e. for each $x$ coordinate found recorded the polyarc, which had a lower upper side. Similarly, we computed a maximal lower envelope. The area above the maximal lower envelope and minimal upper envelope is the intersection of two polyarcs. Each of these steps can be performed in linear time by a sweep line technique.

The envelope representation of polyarcs is also helpful for querying whether a point is inside the polyarc or no. One needs to simply do a binary search for the $x$ coordinate of the candidate point.
If a vertical line at that location crosses the higher envelope on a larger value of $y$ than the lower envelope, then the query point is inside the polyarc.

Overall, we wrote more than 1200 lines of code in C++ for the implementation of the algorithms, and 800 lines of code for testing. Large part of the code is dedicated to the closeness algorithm in $O(n \log^2 n)$. We used C++ for the implementation so that the overhead that the language adds to the runtime is minimized. A simple language to write code in is Python, but we think that in this case the strong type checking of C++ helped us, because of a complex structure of the code. 

\section{Experiments}
\label{sec:experiments}
- mention that we have used random walk to mimic reality. in that case the good algorithm for monotonicity is not that good. but for the worst case, it's great.

- 

\section{Conclusion}
\label{sec:conclusion}

We will describe their algorithm for monotonicity property, which due to its simplicity does not require a divide and conquer in the second stage of the algorithm.

\section{Our contribution}
We've implemented
\begin{itemize}
\item In $O(n)$ time  we  can  find  all  maximal  subsequences  that  define  monotone  paths  in  some (subpath-dependent) direction. \cite{bokal_et_al:LIPIcs:2015:5113} 
\item In $O(n \log^2 n)$ time time we can find all maximal subsequences with diameter at most 1. \cite{chan_et_al:LIPIcs:2016:5920}
\end{itemize}

\section{Algorithms}
\subsection{$k^*$}

Let $k^*(i) = \inf_{m \geq i} \{d(i, m) > 1\}$. \textbf{Claim} $j^*(i-1) = \min(j^*(i), k^*(i-1))$. Thus after we calculate $k^*(i)$ for all elements, we can calculate $j^*(i)$ in $O(n)$ time by looping over all indices in the reverse order.

\subsection{Bokal et al Overview}
TODO: A page that deftly describes the algorithm we've implemented.

\subsection{Chan, Prat Overview}
TODO: A page that deftly describes the algorithm we've implemented.

\section{Implementation Details}
In this section we discuss some implementation details of the Chan, Pratt algorithm for diameter query problem. We skip the details of the monotonicity query, because it was relatively simple.

The novelty of the algorithm from Chan, Pratt is how they combine 1D range tree, with secondary structure that stores circle intersections. While the algorithm is elegant, its implementation is not. In particular, it is necessary to implement an intersection algorithm for arbitrary \textit{polyarcs}. A polyarc is a convex object, similar to a polygon, but instead of straight edges each side is an arc of a circle. The algorithm for the intersection of polyarcs is similar to that of polygons, however there are many more edge cases to consider.

\section{Experimental Results}

\subsection{Monotonicity}

TODO: Describe Naive algorithm which is a $O(n^2)$ algorithm.

\begin{figure}[!ht]
  \centering
  \includegraphics[height=8cm]{../plots/monotonicity_moving_gaussian}
  \caption{Monotonicity algorithm results for moving point with Gaussian noise.}
  \label{fig:monotonicity_demo}
\end{figure}

First let's look at the monotonicity resutls. Here we use the following dataset.

We have a point moving along $x$ axis in the positive direction with speed 1 (i.e. 1 per index), we add Gaussian noise to this point to make the problem interseting. In Figure \ref{fig:monotonicity_demo} you can see results for noise with standard deviations $\sigma_y = 0.05, \sigma_y = 1$. After we generate the dataset, we run Bokal et. al algorithm on the dataset and the answer boundarries of the matrix $A$ that indicates if for points in range $[i, j]$ exists a common direction that has positive dot product with each of the displacements.

Figure \ref{fig:monotonicity_comparison} shows runtime differences in millliseonds between Naive algorithm and algorithm we presented from Bokal et al.
\begin{figure}[!ht]
  \centering
  \includegraphics[height=5cm]{../plots/monotonicity_comparison}
  \caption{Runtimes (in millseconds) of Naive and Bokal et. al}
  \label{fig:monotonicity_comparison}
\end{figure}

\subsection{Diameter}

Now the dataset is a random walk starting at the origin. At each step we uniform randomly pick a direction, and move 0.3 distance in that direction. We keep doing this until we have enough points for an experiment.

TODO: Describe Naive algorithm which is a $O(n^2)$ algorithm.

\begin{figure}[!h]
  \centering
  \includegraphics[height=8cm]{../plots/diameter_random_walk}
  \caption{Caption...}
  \label{fig:diameter_demo}
\end{figure}

Figure \ref{fig:diameter_demo} shows boundaries of points that satisfy all points in range $[i, j]$ fall into some circle with radius 1.

Figure \ref{fig:diameter_comparison} will show results comparing runtimes of Naive algorithm with algorithm we presented from Bokal et al.
\begin{figure}[!ht]
  \centering
  \includegraphics[height=5cm]{../plots/diameter_comparison}
  \caption{Runtimes (in millseconds) of Naive and Chan, Prat [Similar to Figure \ref{fig:monotonicity_comparison}}
  \label{fig:diameter_comparison}
\end{figure}

\section{Conclusion}
It was really challanging to do an implementation project in computational geometry, some sentences in the papers take 100+ lines of code to implement. Thus, for small datasets, added complexity is not worth the speedup gained by the algorithm.

This was a great project otherwise, we've implemented couple of algorithms that we have seen during the lecture - sweep line, range tree, intersection of convex polygons using envelopes. Also couple that we had not found in Bokal et al and Chan, Prat.

\bibliographystyle{unsrt}
\bibliography{papers}
\end{document}
